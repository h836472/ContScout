#!/usr/bin/Rscript
#NC Revision#2 official version, 12th Sep 2023, 13:20
byedie=function(x,log=TRUE,sure_q=TRUE) #set FALSE for development, TRUE for production
{
cat(x)
if(log)
{
writeLines(x,con=logfile)
}
if(sure_q)
{
q(save="no",status=1)
}else
{
cat("\nFatal error found but quit cancelled by sure_q=FALSE...\n")
}
}

info=function(x,log=T){
cat(paste(x,"\n"))
if(log)
{
writeLines(x,con=logfile)
}
}

progname="ContScout"
info("This is ContScout, a contamination remover tool written in R.\n",log=F)
info("Loading R libraries.\n",log=F)
suppressPackageStartupMessages(library("optparse"))
suppressPackageStartupMessages(library("parallel"))
suppressPackageStartupMessages(library("bitops"))
suppressPackageStartupMessages(library("GenomicRanges"))
suppressPackageStartupMessages(library("rtracklayer"))
suppressPackageStartupMessages(library("WriteXLS"))

cargs=commandArgs(trailingOnly=T)
option_list = list(
  make_option(c("-a", "--aligner"), type="character", default="mmseqs", 
              help="Aligner software to use", metavar="aligner"),
  make_option(c("-u", "--userdir"), type="character", default=NULL, 
              help="Path to local database repository folder", metavar="userdir"),
  make_option(c("-U", "--unknown"), type="character", default="keep", 
              help='How to handle proteins without reference hit ("keep" / "drop")', metavar="unknown"),
  make_option(c("-w", "--what"), type="character", default=NULL, 
              help="Main project name", metavar="what"),
  make_option(c("-c", "--cpu"), type="character", default="all", 
              help="Number of CPUs to use", metavar="cpu"),
  make_option(c("-C", "--consensus"), type="character", default="0.5", 
              help="Consensus taxon call threshold", metavar="consensus"),
  make_option(c("-d", "--dbname"), type="character", default=NULL,help="Name of the reference database",metavar="dbname"),
  make_option(c("-i", "--inputdir"), type="character", default=NULL, 
              help="Input data directory containing fasta protein sequence and annotation files", metavar="inputdir"),
  make_option(c("-q", "--querytax"), type="character", default=NULL, 
              help="NCBI TaxonID for query", metavar="querytax"),
  make_option(c("-f", "--force"), action="store_true", default=FALSE, 
              help="Force the analysis despite inconsistencies between annotation file and fasta file."),
  make_option(c("-G", "--genome_filter"), action="store_true", default=FALSE, 
              help="Remove contaminants from genome too."),
  make_option(c("-r", "--reuse_abc"), action="store_true", default=FALSE,metavar="reuse_abc", 
              help="Reuse ABC file from previous run if possible."),
  make_option(c("-n", "--no_annot"), action="store_true", default=FALSE,  
              help="Perform analysis on a genome with no annotation."),
  make_option(c("-N", "--num_hits"),  type="numeric", default=100,  
              help="Consider up to this many database hits per query protein."),
  make_option(c("-s", "--sensM"), type="character", default="2", 
              help="Sensitivity value for MMSeqs", metavar="sensM"),
  make_option(c("-S", "--sensD"), type="character", default="fast", 
              help="Sensitivity value for Diamond", metavar="sensD"),
  make_option(c("-m", "--memlimit"), type="character", default=NULL, 
              help="Memory usage limit for ", metavar="memlimit"),
  make_option(c("-l", "--list_databases"), action="store_true", default=FALSE,
              help="List pre-formatted databases stored in the local database repository."),
  make_option(c("-p", "--pci"), type="character", default="20",
              help="Minimum percentage of sequence identity between query and hit.",metavar="pci"),
  make_option(c("-t", "--tmpdir"), type="character", default=NULL,
             help="Path to temp folder.",metavar="tmpdir"));
opt_parser = OptionParser(option_list=option_list,prog=progname);

if(FALSE)
{
#this section is placed to allow manually configured options, if needed for interactive run during development
opt=list()
opt[["aligner"]]="mmseqs" # -a 
opt[["userdir"]]="/work/balintb/databases_dev/" #-u 
opt[["cpu"]]=34 #-c 
opt[["dbname"]]="uniref100" #-d 
opt[["inputdir"]]="/node7_data/balintb/ContScout_Revision/Revision#2/Effect_of_N/Salprose" # -i
opt[["outdir"]]="./"
opt[["querytax"]]=946362 #-q 
opt[["force"]]=FALSE
opt[["num_hits"]]=100
opt[["reuse_abc"]]=TRUE #-r
opt[["no_annot"]]=FALSE
opt[["sensM"]]=2 #-s 2
opt[["sensD"]]="fast" #-S fast
opt[["memlimit"]]="600G" #-m 150G
opt[["list_databases"]]=FALSE #
opt[["pci"]]=20 #-p 20
opt[["tmpdir"]]="/tmp" #-t 
opt[["consensus"]]=0.5
opt[["genome_filter"]]=FALSE
opt[["unknown"]]="keep"
}else{
opt = parse_args(opt_parser);
}
startdir=getwd()
time_start=Sys.time()
logfilename=paste0(startdir,"/ContScout_",format(time_start,"%d%b_%Y_%H_%M"),".log")
opt[["logfilename"]]=logfilename
logfile=file(logfilename,open="a") 

offsets=1:6
names(offsets)=c("family","order","class","phylum","kingdom","superkingdom")

#Tool detects the number of physical cores, IGNORING hyperthreading

cpu_info=system("lscpu",intern=T)
numT=grep('Thread(s) per core:',cpu_info,fixed=T,value=T)
numT=as.numeric(gsub("^.+\\s","",numT))
numC=grep("^CPU\\(s\\):",cpu_info,value=T)
numC=as.numeric(gsub("^CPU\\(s\\):\\s+","",numC))
numC_max=numC/numT

if(opt[["cpu"]]=="all")
{
opt[["cpu"]]=numC_max
} else if(as.numeric(opt[["cpu"]])>as.numeric(numC_max))
{
info(paste0("System only has ",numC_max, " CPUs installed. Decreasing -c value accordingly.\n"),log=T)
opt[["cpu"]]=numC_max
}

if(!tolower(opt[["unknown"]])%in%c("keep","drop"))
{
byedie(paste0("Please set,how unknown proteins shall be treated. Accepted -U options: ",'"',"keep",'"'," or ",'"',"drop",'"',".\n"))
}

if(!grepl("^\\d+$",opt[["num_hits"]]))
{
byedie(paste0("Please set the number of maximum hits to look for per each query sequence."))
}
opt[["num_hits"]]=as.numeric(opt[["num_hits"]])

if(opt[["num_hits"]]>=300)
{
info(paste0("Requesting for many (-N ",opt[["num_hits"]],") hits per query sequence. Runtime and disk storage might suffer.\n"),log=T)
}

if(as.numeric(opt[["num_hits"]])<=50)
{
info(paste0("Requesting for too few (-N ",opt[["num_hits"]],") hits per query sequence. Screening performance might suffer.\n"),log=T)
}

#user dir check. Reference databases are kept in this directory, created by updateDB
if(is.null(opt[["userdir"]]))
{
byedie(paste0("Please provide the path to your user directory (-u) containing the pre-formatted reference databases.\nIf you do not have any local database yet, you can use the tool \'updateDB\'.","\n"))
}

#detect reference databases
databases=list.files(opt[["userdir"]],pattern="\\.dbinfo",recursive=T,full.names=T)
names(databases)=gsub("\\.dbinfo$","",gsub("^.+/","",databases))
names(databases)=gsub("_",":",names(databases))
if(length(databases)==0)
{
byedie(paste0("No reference database was found under ",'"',opt[["userdir"]],'"',".","Please check if the local directory is properly set.\nRemember, you can download and pre-format reference databases with ","'","UpdateDB",'"',".\n"))
}
dbdata=lapply(databases,function(x) readLines(x))

#if -l switch is present, tool lists all configured reference databases and quits

if (opt[["list_databases"]])
{
cat("Listing pre-formatted reference databases.\n")
res=lapply(names(dbdata),function(x) writeLines(c(paste0("### ",x," ###"),dbdata[[x]],"\n")))
cat(paste0("Please select a database. Examples: ",'"',"-d refseq",'"',", ",'"',"-d refseq:latest",'"',",",'"',"-d refseq:04f69a06",'"',"\n"))
q(save="no",status=0)
}

#ensure, user sets the database name (example -d uniprotKB)
if(is.null(opt[["dbname"]]))
{
byedie("Please provide the name of the reference database (-d) you wish to search against.\nYou can use -l switch to list locally installed reference databases.\n")
}
dbname=unlist(strsplit(opt[["dbname"]],":"))
if(length(dbname)==1 || grepl("^latest$",dbname[[2]],ignore.case=T))
{
db.sel=databases[grep(dbname[[1]],names(databases))]
db.sel.timestamp=sapply(db.sel,function(x) gsub("^Creation_Date: ","",grep("Creation_Date",readLines(x),value=T)))
db.sel.timestamp=as.POSIXct(db.sel.timestamp,format="%Y-%m-%d_%H:%M:%S")
db.latest=names(db.sel[which(db.sel.timestamp==max(db.sel.timestamp))])
db.sel.data=dbdata[[db.latest]]
}else
{
db.sel.data=dbdata[[opt[["dbname"]]]]
if(is.null(db.sel.data))
{
byedie(paste0("Could not find database",'"',opt[["dbname"]],'"'," at location ",'"',opt[["userdir"]],'"',".\n"))
}
}

db.sel.data=lapply(db.sel.data,function(x) unlist(strsplit(x,": ")))
db.sel.data=do.call(rbind,db.sel.data)
colnames(db.sel.data)=c("Tag","Value")
rownames(db.sel.data)=db.sel.data[,1]

#ensure, user defines an input dir. Query genome shall be copied there by the user. 
#input dir shall contain a "protein_seq" and an "annotation_data" subfolders containing predicted protein sequence and genome annotation data, respectively

if (is.null(opt[["inputdir"]]))
{
byedie("Please specify an input directory that contains the protein sequence and annotation file!\n")
}

if (!dir.exists((opt[["inputdir"]])))
{
byedie(paste0("Could not open input directory at ",'"',opt[["inputdir"]],'"',"!\n"))
}

setwd(opt[["inputdir"]])


#tmpdir check and set
if(is.null(opt[["tmpdir"]]))
{
opt[["tmpdir"]]=Sys.getenv("TMPDIR")
if(nchar(opt[["tmpdir"]])==0)
{
opt[["tmpdir"]]="/tmp"
}
}
info(paste0("Temporary dir set to:",opt[["tmpdir"]],"."))

if(!file.exists(opt[["tmpdir"]]))
{dir.create(opt[["tmpdir"]])}

if(opt[["aligner"]]=="diamond" && !tolower(opt[["sensD"]])%in%c("mid-sensitive","sensitive","more-sensitive","very-sensitive","ultra-sensitive","fast"))
{
byedie("Please set a valid Diamond sensitivity value (default: fast).")
}

#memlimit format check
if (!is.null(opt[["memlimit"]])&&!grepl("^\\d+G$",opt[["memlimit"]]))
{
byedie(paste0('Please provide a memory limit in {num}G format. Example: 150G!\n'))
}

#aligner check
opt[["aligner"]]=tolower(opt[["aligner"]])
if(!opt[["aligner"]]%in%c("mmseqs","diamond"))
{
byedie("Please select MMSeqs (-a mmseqs) or Diamond (-a diamond) as your preferred aligner!\n")
}

#ensure, user sets an NCBI taxon ID for the query genome. That sets the taxon information for the host, that shall be cleaned.
if(is.null(opt[["querytax"]])){
byedie(paste0('Please provide an NCBI Taxon ID for the query genome via option -q!',"\n"))
}else if(!grepl("^\\d+$",opt[["querytax"]]))
{
byedie(paste0('Please ensure the NCBI Taxon ID only contains numeric characters!'))
}

#checking for the presence of required input files (protein sequence, plus genome annotation data)
protfile=list.files(paste0("protein_seq"),full.name=T)
protfile=grep("(\\.fa\\.*g*z*$|\\.faa.*g*z*$|\\.fasta.*g*z*$)",protfile,value=T,ignore.case=T)
if(!length(protfile)==1)
{
byedie(paste0('Please create a folder named "protein_seq" within ',opt[["inputdir"]]," and copy your protein fasta file there.\nPlease note that only one protein file is allowed per run."))
}

if(opt[["genome_filter"]])
{
dnafile=list.files(paste0("dna_seq"),full.name=T)
dnafile=grep("(\\.fa\\.*g*z*$|\\.fna.*g*z*$|\\.fasta.*g*z*$)",dnafile,value=T,ignore.case=T)
if(!length(dnafile)==1)
{
byedie(paste0('To perform genome filtering, please create a folder named "dna_seq" within ',opt[["inputdir"]]," and copy your DNA fasta file there.\nPlease note that only one DNA file is allowed per run."))
}
}

if(!opt[["no_annot"]] || opt[["genome_filter"]])
{
annotfile=list.files(paste0("annotation_data"),full.name=T)
annotfile=grep("\\.g[tf]f3*\\.*g*z*$",annotfile,value=T,ignore.case=T)
if(!length(annotfile)==1)
{
byedie(paste0('Please create a folder named "annotation_data" within ',opt[["inputdir"]]," and copy your GTF/GFF annotation file there.\nPlease note that only one annotation file is allowed per run."))
}
}else{
info("Option -n active, run is carried out witout an annotation file.\nEach protein will be assigned into a virtual singleton contig.\n")
}
#check if the threshold for consensus contig taxon call is set properly. It shall be ]0.5-0.9999)
opt[["consensus"]]=as.numeric(opt[["consensus"]])
if(is.na(opt[["consensus"]]) || opt[["consensus"]]<0.5 || opt[["consensus"]] >(1-1e-4))
{
byedie(paste0("Please set a valid consensus vote therhold for contig taxon call via parameter -C.\nThis value shall be between 0.5 and 0.9999. Default value is 0.5"))
}
#Tool automatically picks up the NCBI database version that was used by updateDB, when formatting the reference database.
taxdb.loc.sel=paste0(opt[["userdir"]],"/",gsub("/diamond/.+$","",db.sel.data["Diamond_DB","Value"]),"/ncbi_tax/",db.sel.data["Tax_CRC","Value"])

info("Pre-processing NCBI taxon database.")

q.tax.cmd=paste0("grep ",opt[["querytax"]]," ",taxdb.loc.sel,"/",db.sel.data["Tax_CRC","Value"],"_rankedlineage.dmp")
q.tax.hit=system(q.tax.cmd,intern=T)
q.tax.hit=grep(paste0("^",opt[["querytax"]],"\\t"),q.tax.hit,value=T)

if(length(q.tax.hit)!=1)
{
byedie(paste0('Could not link TaxonID "',opt[["querytax"]],'" to a single taxon tag within the Taxon Database. It is either missing or present in multiple copies.'))
}else{
 q.tax.hit=gsub("\\t\\|$","",q.tax.hit)
 tax.last=gsub("^.+\\t","",q.tax.hit)
 tax.first=gsub(" ","_",gsub("\\t\\|\\t.+$","",gsub("^\\d+\\t\\|\\t","",q.tax.hit)))
 tax.first=gsub("[\\*,\\/,\\\\,\\',?,\\^,\\$,\\@,\\#,\\`,\\,\\>,\\<,\\:,\\&,\\{,\\},\\),\\(,\\!,\\+,\\=,\\%,\\ ́,\\ ̌,\\;]","_",tax.first) #remove chars from taxon name that would mess up folder name when used as part of outdir
 tax.first=gsub('\"',"_",tax.first)
 tax.first=gsub("(\\[|\\])","",tax.first)
 tax.first=gsub("_+","_",tax.first)
 if(tax.last%in%c("Archaea","Bacteria","Viruses"))
  {
  q.tax.tag=tax.last
  }else if(tax.last=="Eukaryota"){
   tax.euk2=unlist(strsplit(q.tax.hit,"\\t\\|\\t"))[[9]]
   if(tax.euk2=="")
    {q.tax.tag="Other_eukaryote"}else{q.tax.tag=tax.euk2}
  }else{
byedie("The provided TaxonID was not found within Archaea, Bacteria or Eukarota.")
}
}
now.stamp = paste0(tax.first,"_tax_",opt[["querytax"]],"_",format(time_start,"%d%b_%Y_%H_%M"))

if(is.null(opt[["what"]]))
{
opt[["what"]]=tax.first
}

tidrl.file=paste0(taxdb.loc.sel,"/",db.sel.data["Tax_CRC","Value"],"_rankedtaxidlineage.RDS")
tax=readRDS(tidrl.file)
rankedlineage=readLines(paste0(taxdb.loc.sel,"/",db.sel.data["Tax_CRC","Value"],"_rankedlineage.dmp"))
rankedlineage=unlist(mclapply(rankedlineage,function(x) gsub("\t|\t","_|_",x,fixed=T),mc.cores=opt[["cpu"]]))
rankedlineage=unlist(mclapply(rankedlineage,function(x) gsub("\t|","",x,fixed=T),mc.cores=opt[["cpu"]]))
rankedlineage=mclapply(rankedlineage,function(x) return(unlist(strsplit(x,"_\\|_"))),mc.cores=opt[["cpu"]])
names(rankedlineage)=unlist(mclapply(rankedlineage,function(x) return(paste0("t",x[[1]])),mc.cores=opt[["cpu"]]))

taxDict=sapply(rankedlineage,function(x) x[[2]])

query_taxline=paste0("t",tax[paste0("t",opt[["querytax"]]),c("family","order","class","phylum","kingdom","superkingdom")])
names(query_taxline)=c("family","order","class","phylum","kingdom","superkingdom")
outdir=paste0(now.stamp)
#for manual debugging, after readRDS("Arguments.RDS")
#outdir=opt[["outdir"]]

opt[["outdir"]]=outdir
#Creating output directory structure
if(!file.exists(outdir))
{dir.create(outdir)
dir.create(paste0(outdir,"/R_saved_objects"))
dir.create(paste0(outdir,"/diag_data"))
dir.create(paste0(outdir,"/filtered_outputs"))
}
saveRDS(query_taxline,paste0(outdir,"/R_saved_objects/Query_taxline.RDS"))

taxline_outtext=c("Query taxon lineage:",paste(names(query_taxline),gsub("^t","",query_taxline),taxDict[query_taxline],sep=":"),"\n")
info(taxline_outtext)

taxtable.file=list.files(pattern="taxID_DBcount.RDS",full.name=T,gsub("/ncbi_tax/.+$","",taxdb.loc.sel))
taxtable=readRDS(taxtable.file)

saveRDS(opt,paste0(outdir,"/R_saved_objects/Arguments.RDS"))
close(logfile) #we close the initial log file and move it to the final outdir folder
rf=file.rename(logfilename,paste0(outdir,"/",opt[["what"]],"_ContScout_",format(time_start,"%d%b_%Y_%H_%M"),".log"))
opt[["logfile"]]=paste0(outdir,"/",opt[["what"]],"_ContScout_",format(time_start,"%d%b_%Y_%H_%M"),".log")
logfile=file(opt[["logfile"]],open="a") 
info(paste0("Analysis started at ",time_start),log=T)
info(paste0("Command: ",paste(cargs,collapse=" ")),log=T)
info("Databases used:",log=T)
info(apply(db.sel.data,1,function(x) paste(x,collapse=": ")))

cat("Now reading fasta headers file.\n")
if(grepl("\\.gz$",protfile)){
input.headers=gsub(">","",system(paste0('zgrep ">" ',protfile),intern=T))
}else{
input.headers=gsub(">","",system(paste0('grep ">" ',protfile),intern=T))
}
input.headers=gsub(" .+","",input.headers) #remove any extra info from fasta headers beyond space character
dir.create(paste0(outdir,"/protein_seq"))
copyres=file.copy("protein_seq",paste0(outdir),recursive=T)

if(!opt[["no_annot"]]){
cat("Now reading annotation file.\n")
GFF=readGFF(annotfile)
prot_ID_column=grep("^protein_*id$",colnames(GFF),ignore.case=T,value=T) #understands proteinID protein_ID in any capitalization
if(length(prot_ID_column)!=1)
{
byedie("Error found in annotation file. After GFF import, there should be exactly one \"protein_id\" column present.\nExiting...\n")
}
GFF.sel=data.frame(GFF[GFF[,prot_ID_column]%in%input.headers,c("seqid",prot_ID_column)],stringsAsFactors=F)
if(nrow(GFF.sel)==0)
{
byedie("Error! Protein IDs in the protein sequence file completely differ from the IDs used in the GFF file.\nExiting...\n")
}
GFF.sel.s=split(GFF.sel,GFF.sel[,prot_ID_column]) 
MultiCtgProts=unlist(mclapply(GFF.sel.s,function(x) return(length(unique(x[,"seqid"]))>1),mc.cores=opt[["cpu"]]))
MultiCtgProts=names(MultiCtgProts)[MultiCtgProts]
gene.contig=mclapply(GFF.sel.s,function(x) return(unique(as.character(x[,"seqid"]))),mc.cores=opt[["cpu"]])
if(!opt[["force"]])
{
if(length(MultiCtgProts)>0) 
{
byedie(paste0("Inconsistent annotation found:\n",length(MultiCtgProts)," out of ",length(input.headers)," proteins belong to multiple contig.\nRefusing to work with inconsistent annotation data...\nPlease repair your annotation file or use the -f switch to force the analysis.\nIf you continue with the invalid annotation file, proteins mapping to multiple contigs will be randomly assigned. Proteins that are not in the annotation file will be dropped.\n"))
}
}else
{
info("Flag -f / --force  is set so tool will work with an inconsistent annotation.\nPlease note that proteins mapping to multiple contigs will be randomly assigned to a single contig.\nProteins not present in the annotation will be dropped.")
gene.contig=lapply(gene.contig,function(x) return(sample(x,1)))
#if the same protein is mapped to multiple contig, randomly pick one location. 
}
gene.contig=unlist(gene.contig)

GhostProts=setdiff(input.headers,names(gene.contig))
writeLines(GhostProts,paste0(outdir,"/GhostProteins.txt"))
if(!opt[["force"]])
{
if(length(GhostProts)>0) 
{
byedie(paste0("Inconsistent annotation found:\n",length(setdiff(input.headers,names(gene.contig)))," out of ",length(input.headers)," proteins are missing from the annotation file.\nRefusing to work with inconsistent annotation data...\nPlease repair your annotation file or use the -f switch to force the analysis.\nIf you continue with the invalid annotation, all unmapped proteins will be marked\nand removed regardless to their taxon flags.\n"))
}
}else
{
info("Flag -f / --force  is set so tool will work with an inconsistent annotation.\nPlease note that any protein that has no record in the annotation will be marked as ghost and will be removed.\n")
#Proteins that are absent from annotation, are removed.
}

ctgDB=cbind(names(gene.contig),gene.contig)
ctgDB=data.frame(ctgDB,stringsAsFactors=F)
colnames(ctgDB)=c("ProteinID","Contig")
rownames(ctgDB)=NULL
dir.create(paste0(outdir,"/annotation_data"))
copyres=file.copy("annotation_data",outdir,recursive=T)
}else
{
info("Found -n / --no_annot switch. Creating a virtual contig database linking each protein as a singleton.\nThat way, proteins are evaluated one by one.\nProteins from HGT events are likely to be tagged as contamination when using this switch.\n")
ctgDB=matrix(ncol=2,nrow=length(input.headers))
colnames(ctgDB)=c("ProteinID","Contig")
ctgDB[,"Contig"]=paste0("NullCtg_",1:length(input.headers))
ctgDB[,"ProteinID"]=input.headers
ctgDB=data.frame(ctgDB,stringsAsFactors=F)
GhostProts=""
}
saveRDS(ctgDB,paste0(outdir,"/R_saved_objects/",opt[["what"]],".ContigDB.RDS"))
#manual debugging 
#ctgDB=readRDS(paste0(outdir,"/R_saved_objects/",opt[["what"]],".ContigDB.RDS"))
#Checking if the alignment step, that is computationally very costly, needs to be done

run_alignment=TRUE

if(opt[["reuse_abc"]])
{
#user asks for ABC re-use. Tool confirms if all needed files are available and intact. If so, alignment step can be skipped.
if(!is.null(opt[["aligner"]]))
{
info(paste0("Info: re-use flag (-r) is set, so ABC file from previous run will be used.\nAlignment software (-a), reference database (-d) and \nsearch sensitivity settings (-s, -S)\nwill be ignored.\n"))
}

if(file.exists("R_saved_objects/ABC.s.RDS")&&file.exists("R_saved_objects/ABC.s.md5sum")&&file.info("R_saved_objects/ABC.s.md5sum")[,"size"]>0){
ABC.saved.md5=readLines("R_saved_objects/ABC.s.md5sum")
ABC.saved.md5=substr(ABC.saved.md5,1,32)
ABC.check.md5=system("md5sum R_saved_objects/ABC.s.RDS",intern=T)
ABC.check.md5=substr(ABC.check.md5,1,32)
 if(!file.exists("RefTaxLookup.ABC"))
 {
 info("ABC data is missing. Repeating the alignment step.\n")
 }else
 {
  if(ABC.check.md5==ABC.saved.md5)
  {
  info("Alignment R Data file from previous run was found intact. Re-using it as requested.\n")
  run_alignment=FALSE
  cat("Now reading Alignment result (ABC) file.\n")
  ABC=read.table("RefTaxLookup.ABC",sep="\t",as.is=T) 
  }else{
  info("MD5 signature mismatch. Alignment R Data file from previous run seems to be corrupt. Repeating the entire alignment step.")
  }
 }
 }else {
 info("ABC data is missing. Repeating the alignment step.")
 }
}

#Alignment is needed. Either, this is a new run, or alignment data from previous run is corrupted and can not be re-used.
if(run_alignment)
{
if(opt[["aligner"]]=="mmseqs")
{
##MMSeqs aligner selected##

#detect the CPU vector instruction set. This is for runtime optimization. Performance: avx2 >> sse4_1 >>sse2
cpu=grep("Flags:",system("lscpu",intern=T),value=T)
if(grepl(" avx2 ",cpu)){
mmseqs.bin="mmseqs_avx2"
} else if (grepl(" sse4_1 ",cpu)) {
mmseqs.bin="mmseqs_sse41"
} else {
mmseqs.bin="mmseqs_sse2"
}
qdb=gsub("(\\.fa|\\.faa|\\.fasta).*g*z*$",".db",protfile)
qdb=gsub(".+/","",qdb)

#create query database for MMSeqs
MM.qdb.cmd=paste0(mmseqs.bin," createdb ",protfile," ",outdir,"/",qdb)
cat("Converting query sequences to MMSeqs database format\n")
system(MM.qdb.cmd)
mm.refDB.loc=paste0(opt[["userdir"]],"/",db.sel.data["MMSeqs_DB","Value"])
MM.search.cmd=paste0(mmseqs.bin," search -s ",opt[["sensM"]]," --max-accept ",opt[["num_hits"]]," --max-seqs ",opt[["num_hits"]]," -a --threads ",opt[["cpu"]]," ",outdir,"/",qdb," ",mm.refDB.loc," ",outdir,"/TaxMMSeqs.resDB"," ",opt[["tmpdir"]])
if(!is.null(opt[["memlimit"]]))
{
MM.search.cmd=paste0(MM.search.cmd," --split-memory-limit ",opt[["memlimit"]])
}
MM.search.cmd=paste0(MM.search.cmd," >",outdir,"/mmseqs.log 2>",outdir,"/mmseqs.error" )

info(paste0("Search command:\n",MM.search.cmd,"\n"))
#perform MMSeqs search
system(MM.search.cmd)
#perform ABC file export
Trafo.cmd=paste0(mmseqs.bin," convertalis ",outdir,"/",qdb," ",mm.refDB.loc," ",outdir,"/TaxMMSeqs.resDB"," ",outdir,"/RefTaxLookup.ABC --format-output query,target,bits,qlen,nident --threads ",opt[["cpu"]])
system(Trafo.cmd)
}else if (opt[["aligner"]]=="diamond"){

##Diamond aligner selected##
diam.refDB.loc=paste0(opt[["userdir"]],"/",db.sel.data["Diamond_DB","Value"])
DM.search.cmd=paste0("diamond blastp -d ",diam.refDB.loc," -q ",protfile," -o ",outdir,"/RefTaxLookup.ABC --max-target-seqs ",opt[["num_hits"]]," -f 6 qseqid sseqid bitscore qlen nident --threads ", opt[["cpu"]]," ",opt[["sens_D"]])
if(!is.null(opt[["memlimit"]]))
{
DM.search.cmd=paste0(DM.search.cmd," --memory-limit ",gsub("G$","",opt[["memlimit"]]))
}
info(paste0("Search command:\n",DM.search.cmd,"\n"))
system(DM.search.cmd)
}
cat("Now reading Alignment result (ABC) file.\n")
ABC=read.table(paste0(outdir,"/RefTaxLookup.ABC"),sep="\t",as.is=T) 
}
colnames(ABC)=c("Q","H","BS","Qlen","Nident")
ABC[,"H"]=gsub("\\|(TaxID|RepID):",":",ABC[,"H"])
ABC[,"QPCI"]=ABC[,"Nident"]/ABC[,"Qlen"]*100
ABC=ABC[ABC[,"QPCI"]>=as.numeric(opt[["pci"]]),]
ABC=ABC[!grepl(":NA$",ABC[,"H"]),] #drop uninformative hits with NA as taxon ID.
ABC[,"H.TaxID"]=unlist(mclapply(ABC[,"H"],function(x) unlist(strsplit(x,":"))[2],mc.cores=opt[["cpu"]]))
ABC[,"Q.TaxID"]=paste0("t",opt[["querytax"]])
ABC=ABC[ABC[,"Q.TaxID"]!=ABC[,"H.TaxID"],] #Remove hits from reference database that could originate from the query of interest (i.e. self hits)
ABC.withtax=ABC[,"H.TaxID"]%in%rownames(tax)

info("Printing statistics on ABC files with (TRUE) or without (FALSE) taxon info.")
summary(ABC.withtax) #we drop hits without 
info("Dropping ABC lines with no taxon info.")
ABC=ABC[ABC.withtax,]
tax.sel=tax[ABC[,"H.TaxID"],c("family","order","class","phylum","kingdom","superkingdom")]  # we remove genus info realizing that this resolution is impossible to use in most cases, including Human sequences
colnames(tax.sel)=paste0("Tax:",colnames(tax.sel))
ABC=cbind(ABC,tax.sel)
ABC.s=split(ABC,ABC[,"Q"])
ABC.s=mclapply(ABC.s,function(x) return(x[order(-x[,"QPCI"]),]),mc.cores=opt[["cpu"]])
ABC.s=lapply(ABC.s,function(x) head(x,n=opt[["num_hits"]]))
saveRDS(ABC.s,paste0(outdir,"/R_saved_objects/ABC.s.RDS"))
system(paste0("md5sum ",outdir,"/R_saved_objects/ABC.s.RDS >",outdir,"/R_saved_objects/ABC.s.md5sum"))

#helper functions for protein classification

top_RLE=function(y,taxdata)
{
#get the taxon tag from the best hit as well as number of consequtive hits supporting the same tag (i.e. run length for first taxon value)
NR_taxRLE=Rle(taxdata[,y])
if(gsub("^.+_","",runValue(NR_taxRLE)[1])!=32644){
return(paste0("t",runValue(NR_taxRLE)[1],":",runLength(NR_taxRLE)[1]))
}else
{
return(paste0("NA:",runLength(NR_taxRLE)[1]))
}
}

topHitStat=function(x)
{
cat(".")
x.tax=x[,grep("^Tax:",colnames(x))]
colnames(x.tax)=gsub("Tax:","",colnames(x.tax))
x.calls=unlist(lapply(colnames(x.tax),function(y) top_RLE(y,taxdata=x.tax)))
}

jaccard = function(a, b) {
#calculate jaccard statistics for protein hit lists (individual candidates vs proteins of alien contigs)
    intersection = length(intersect(a, b))
    union = length(a) + length(b) - intersection
    return (intersection/union)
}

info("Performing protein classification.")

protTaxCalls=lapply(ABC.s,FUN=topHitStat)
cat("\n")
protTaxCalls=do.call(rbind,protTaxCalls)
colnames(protTaxCalls)=c("family","order","class","phylum","kingdom","superkingdom")

#Proteins with no hit against the reference database receive NA:0 calls.
empty=c("NA:0","NA:0","NA:0","NA:0","NA:0","NA:0")

missprots=setdiff(input.headers,rownames(protTaxCalls))
if(length(missprots)>0)
{
miss_mat=matrix(rep(empty,length(missprots)),nrow=length(missprots),byrow=T)
colnames(miss_mat)=colnames(protTaxCalls)
rownames(miss_mat)=missprots
protTaxCalls=rbind(protTaxCalls,miss_mat)
}
call_summarizer=function(x)
{
gb=list()
tax=gsub(":.+$","",x)
depth=as.numeric(gsub("^.+:","",x))
s=split(depth,tax)
sl=sapply(s,function(x) length(x))
sl=sl[order(-sl)]
s=s[order(-sl)]
gb[["stats"]]=t(t(sl))
gb[["med_depths"]]=sapply(s,function(y) median(y))
return(gb)
}
res=apply(protTaxCalls[,1:6],2,FUN=call_summarizer)
depths=unlist(lapply(res,function(x) x$med_depths))
names(depths)=gsub("^.+\\.","",names(depths))
stats=lapply(res,function(x) x$stats)

top6_summarizer=function(x)
{
#summary of the top6 most abundant tags for each taxon rank
x=data.frame(x,stringsAsFactors=F)
colnames(x)="Count"
x.sum=sum(x[,1])
x.head=head(x,n=6)
x.headsum=sum(x.head[,1])
x.restsum=x.sum-x.headsum
x.restline=data.frame(x.restsum,stringsAsFactors=F)
rownames(x.restline)="Other"
colnames(x.restline)="Count"
x.head=rbind(x.head,x.restline)
x.head[,"Percentage"]=round(x.head[,"Count"]/sum(x.head[,"Count"])*100)
x.head[,"MedRLE_Len"]=depths[rownames(x.head)]
data.out=x.head
data.out=cbind("NA",data.out)
colnames(data.out)[1]="TaxonName"
data.out=cbind("NA",data.out)
colnames(data.out)[1]="TaxID"
rownames(data.out)[rownames(data.out)=="NA"]="No_data"
data.out[rownames(data.out)=="No_data",c("MedRLE_Len")]="NA"
data.out[rownames(data.out)=="Other",c("MedRLE_Len")]="NA"
data.out[,"TaxID"]=gsub("^t","",rownames(data.out))
#we convert NCBI Taxon IDs to taxon names
data.out[grep("^t\\d+",rownames(data.out)),"TaxonName"]=as.character(taxDict[grep("^t\\d+",rownames(data.out),value=T)])
return(data.out)
}
stats_data_out=lapply(stats,FUN=top6_summarizer)
xls_out=paste0(outdir,"/diag_data/",opt[["what"]],"_Top6_TaxonProtTags.xlsx")

info("Exporting taxon tag statistics on individual proteins.")
WriteXLS(stats_data_out,ExcelFileName=xls_out)

ctgDB=cbind(ctgDB,protTaxCalls[ctgDB[,"ProteinID"],])
saveRDS(ctgDB,paste0(outdir,"/R_saved_objects/ctgDB_with_taxCalls.RDS"))
ctgDB.s=split(ctgDB,ctgDB[,"Contig"]) #94
ctgDB.sl=sapply(ctgDB.s,function(x) nrow(x))
ctgDB.sl=cbind(names(ctgDB.sl),ctgDB.sl)
colnames(ctgDB.sl)=c("Ctg.ID","NProt")

adjust_counts=function(x)
{
#this function performs vote weight. If there are many reference proteins backing up the taxon call, query gets a score close to 2. Otherwise, it gets a score close to 1. 
if(x==0)
{
return(0)
}else{
return(2-1/x) 
}
}

CTG_caller=function(x)
{
tags=gsub(":\\d+","",x)
counts=as.numeric(gsub("^.+:","",x))
counts2=sapply(counts,FUN=adjust_counts)
tags.s=split(counts2,tags)

if(opt[["unknown"]]=="drop" && "NA"%in%names(tags.s))
{
#if requested, user can consider "NA" taxon calls in the consensus contig call. By default, they are ignored and only the votes of proteins with reference taxon information matter.
tags.s[["NA"]]=tags.s[["NA"]]+1 #we set the NA count data to 1, so that "unknown" proteins will be considered as a mismatching taxon tag
}

tags.sum=sapply(tags.s,function(x) sum(x))
bigsum=sum(tags.sum) #sum score for all considered taxon tags. 
ratios=tags.sum/bigsum
ratios.sel=ratios[ratios>as.numeric(opt[["consensus"]])] #consensus taxon tag selection. Consensus means > than opt[["consensus"]] threshold parmeter: -C, values: 0.5 < x <= 0.9999 
if(length(ratios.sel)==1){
return(c(names(ratios.sel),ratios.sel))
}else{
return(c(NA,NA))
}
}
info("Performing scaffold (contig) taxon calls.")
res=lapply(ctgDB.s,function(t) lapply(c("family","order","class","phylum","kingdom","superkingdom"),function(x) CTG_caller(t[,x])))
ressum=lapply(res,function(x) sapply(x,function(y) return(y[[1]])))
ressum=do.call(rbind,ressum)
rescall=t(apply(ressum,1,function(x) as.numeric(x==query_taxline)))
if(opt[["unknown"]]=="drop")
{rescall[is.na(rescall)]=0}
rescall=cbind(rownames(rescall),rescall)
colnames(rescall)=c("Ctg.ID","family","order","class","phylum","kingdom","superkingdom")

rescall=merge(rescall,ctgDB.sl,all.x=T,all.y=T,by="Ctg.ID")
rescall=rescall[order(-as.numeric(rescall[,"NProt"])),]

rescall_tag=apply(rescall,1,function(x) return(paste(x[2:7],collapse="")))
rescall_tag.s=summary(factor(rescall_tag))
tagdir=data.frame(rescall_tag.s)
tagdir=cbind(tagdir,rownames(tagdir))
tagdir=tagdir[order(-tagdir[,1]),]

#creating a result call matrix. 1 means taxon is supported. 0 means taxon conflict between contig and host. NA means non-classified contig, without any consensus tag.
rescall_outfile=paste0(outdir,"/diag_data/",opt[["what"]],"_ContigTaxSupportData.xlsx")
rescall_outdata=list("ContigTaxonSupport"=rescall)
WriteXLS(rescall_outdata,rescall_outfile)

saveRDS(rescall,paste0(outdir,"/R_saved_objects/rescall.RDS"))
saveRDS(ctgDB.s,paste0(outdir,"/R_saved_objects/ctgDB.s.RDS"))
tagdir[,"PC"]=round(tagdir[,"rescall_tag.s"]/sum(tagdir[,"rescall_tag.s"])*100,digits=2)
colnames(tagdir)=c("NumContig","TaxSupportFlag","PC")
tagdir=tagdir[,c("NumContig","PC","TaxSupportFlag")]

rescall[is.na(rescall)]=3 #replace NA, meaning no data into 3, so that ==0 comparison is meaningful

#support factor coding: 1 means query and host taxon matches. 2 means query and host taxon mismatches. 3 means there is no taxon information available for query

rescall[,"ContigSupport"]=apply(rescall[,2:7],1,function(y) paste0(y,collapse=""))
rownames(rescall)=rescall[,"Ctg.ID"]
ABC1.s=lapply(ABC.s,function(y) y[1,])
ABC1.t=do.call(rbind,ABC1.s)
ABC1.tax=ABC1.t[,grep("Tax:",colnames(ABC1.t))]
colnames(ABC1.tax)=gsub("Tax:","",colnames(ABC1.tax))
ABC1.call=apply(ABC1.tax,1,function(y) paste(as.numeric(y==gsub("^t","",query_taxline)),collapse=""))
rownames(ctgDB)=ctgDB[,"ProteinID"]
ctgDB[,"ProtSupport"]=ABC1.call[rownames(ctgDB)]
ctgDB[is.na(ctgDB[,"ProtSupport"]),"ProtSupport"]=333333
ctgDB.s=split(ctgDB,ctgDB[,"Contig"]) 
ctgDB.sl=sapply(ctgDB.s,function(y) nrow(y))
prot.callstats=summary(factor(ctgDB[,"ProtSupport"]),maxsum=nrow(ctgDB))
prot.callstats=(prot.callstats[order(-prot.callstats)])
if(!"333333"%in%names(prot.callstats))
{unknownprot.ratio=0}else{
unknownprot.ratio=round(prot.callstats["333333"]/sum(prot.callstats)*100,digits=2)
}
#Throw a warning if the ratio of unclassified proteins is high. That can affect ContScout ability to detect contamination.

if(unknownprot.ratio>50)
{info("Warning! More than 50% of the query proteins have no related sequence in the reference database!") }else if((unknownprot.ratio>25))
{info("Warning! More than 25% of the query proteins have no related sequence in the reference database!") }
saveRDS(prot.callstats,paste0(outdir,"/R_saved_objects/prot.callstats.RDS"))

drop_contigs=lapply(c("family","order","class","phylum","kingdom","superkingdom"),function(y) return(rownames(rescall)[rescall[,y]==0]))
names(drop_contigs)=c("family","order","class","phylum","kingdom","superkingdom")
drop.ctgDB=lapply(drop_contigs,function(y) ctgDB[ctgDB[,"Contig"]%in%y,])
keep.ctgDB=lapply(drop_contigs,function(y) ctgDB[!ctgDB[,"Contig"]%in%y,])
saveRDS(drop.ctgDB,paste0(outdir,"/R_saved_objects/drop_contig_DB.RDS"))

ABC.drop=lapply(names(drop.ctgDB),function(y){prots=drop.ctgDB[[y]][,"ProteinID"];return(ABC.s[names(ABC.s)%in%prots])})
names(ABC.drop)=names(drop.ctgDB)
saveRDS(ABC.drop,paste0(outdir,"/R_saved_objects/ABC_data_for_dropped.RDS"))

RLE_depths=ctgDB[,c("family","order","class","phylum","kingdom","superkingdom","ProtSupport")]
for(i in 1:(ncol(RLE_depths)-1))
{
RLE_depths[,i]=as.numeric(gsub("^.+:","",RLE_depths[,i]))
}

RLE_depths=split(RLE_depths[,1:6],RLE_depths[,7])
RLE_depths=lapply(RLE_depths,function(x) {d=lapply(c("family","order","class","phylum","kingdom","superkingdom"),function(y) return(x[,y]));names(d)=c("family","order","class","phylum","kingdom","superkingdom");return(d)})
RLE_depths=unlist(RLE_depths,recursive=F)

names(RLE_depths)=paste0(names(RLE_depths),":")
RLE_depths=unlist(RLE_depths)
names(RLE_depths)=gsub("^.+\\.","",gsub(":.*$","",names(RLE_depths)))
RLE.s=split(RLE_depths,names(RLE_depths))
RLEmed.s=lapply(RLE.s,function(y) median(y))
RLE.s_no_null=lapply(RLE.s,function(y) return(y[y>0]))
RLEmed.s=lapply(RLE.s,function(y) median(y))
RLEmed_no_null.s=lapply(RLE.s_no_null,function(y) median(y))

info("Calculating diagnostic summaries for each taxon level.")

do.stats=function(x)
{
#creating a summary statistics that shall guide the user when assessing filtering performance of CS at different taxon ranks
indiv_protsupport.x=substr(ctgDB[,"ProtSupport"],offsets[x],offsets[x])
if(opt[["unknown"]]=="keep"){
#default, permissive approach. Keeping unknown contigs
indiv_protdrop.x=ctgDB[indiv_protsupport.x==0,"ProteinID"]
indiv_protkeep.x=ctgDB[indiv_protsupport.x%in%c(1,3),"ProteinID"] 
}else{
#this is strict approach, Dropping unknown contigs. 
indiv_protdrop.x=ctgDB[indiv_protsupport.x%in%c(0,3),"ProteinID"] 
indiv_protkeep.x=ctgDB[indiv_protsupport.x==1,"ProteinID"]
}
indiv_tags.dropped.x=sapply(ABC1.s[indiv_protdrop.x],function(y) return(paste0("t",y[,paste0("Tax:",x)])))
indiv_tags.kept.x=sapply(ABC1.s[indiv_protkeep.x],function(y) return(paste0("t",y[,paste0("Tax:",x)])))
indiv_tags.dropped.x[indiv_tags.dropped.x=="t"]="noABCdata"
indiv_tags.kept.x[indiv_tags.kept.x=="t"]="noABCdata"
ctgdrop.x=drop.ctgDB[[x]]
ctgkeep.x=keep.ctgDB[[x]]
contig_protdrop.x=ctgdrop.x[,"ProteinID"]
contig_protkeep.x=ctgkeep.x[,"ProteinID"]
ctg_tags.dropped.x=sapply(ABC1.s[contig_protdrop.x],function(y) return(paste0("t",y[,paste0("Tax:",x)])))
ctg_tags.kept.x=sapply(ABC1.s[contig_protkeep.x],function(y) return(paste0("t",y[,paste0("Tax:",x)])))

ctg_tags.dropped.x[ctg_tags.dropped.x=="t"]="noABCdata"
ctg_tags.kept.x[ctg_tags.kept.x=="t"]="noABCdata"

tagsummary=list()
tagsummary[["indiv_prot_dropped"]]=summary(factor(indiv_tags.dropped.x),maxsum=length(indiv_tags.dropped.x))
tagsummary[["indiv_prot_kept"]]=summary(factor(indiv_tags.kept.x),maxsum=length(indiv_tags.kept.x))
tagsummary[["ctg_prot_dropped"]]=summary(factor(ctg_tags.dropped.x),maxsum=length(ctg_tags.dropped.x))
tagsummary[["ctg_prot_kept"]]=summary(factor(ctg_tags.kept.x),maxsum=length(ctg_tags.kept.x))
tagsummary=lapply(tagsummary,function(x) return(x[order(-x)]))

tagsummary[["mix_tags"]]=intersect(ctg_tags.dropped.x,ctg_tags.kept.x)
if(length(tagsummary[["mix_tags"]])>0)
{
tagsummary[["ctg_mix_dropped"]]=tagsummary[["ctg_prot_dropped"]][names(tagsummary[["ctg_prot_dropped"]])%in%tagsummary[["mix_tags"]]]
tagsummary[["ctg_mix_dropped"]]=tagsummary[["ctg_mix_dropped"]][tagsummary[["mix_tags"]]]
tagsummary[["ctg_mix_kept"]]=tagsummary[["ctg_prot_kept"]][names(tagsummary[["ctg_prot_kept"]])%in%tagsummary[["mix_tags"]]]
tagsummary[["ctg_mix_kept"]]=tagsummary[["ctg_mix_kept"]][tagsummary[["mix_tags"]]]
tagsummary[["tagmatrix"]]=data.frame(cbind(tagsummary[["ctg_mix_dropped"]],tagsummary[["ctg_mix_kept"]],round(tagsummary[["ctg_mix_dropped"]]/(tagsummary[["ctg_mix_dropped"]]+tagsummary[["ctg_mix_kept"]]),digits=2)),stringsAsFactors=F)
colnames(tagsummary[["tagmatrix"]])=c("Dropped","Kept","Ratio")
tagsummary[["tagmatrix"]]=tagsummary[["tagmatrix"]][!rownames(tagsummary[["tagmatrix"]])%in%c("noABCdata","t32644"),]
}else
{
tagsummary[["ctg_mix_dropped"]]=0
tagsummary[["ctg_mix_kept"]]=0
tagsummary[["tagmatrix"]]=data.frame(matrix(nrow=0,ncol=3))
colnames(tagsummary[["tagmatrix"]])=c("Dropped","Kept","Ratio")
}
saveRDS(tagsummary,paste0(outdir,"/R_saved_objects/",x,".tagsummary.RDS"))
giveback=list()

any_dropped=union(indiv_protdrop.x,contig_protdrop.x)

giveback[["Jaccard"]]=round(jaccard(a=indiv_protdrop.x,b=contig_protdrop.x),digits=2)
giveback[["NumProts"]]=nrow(ctgDB)
taxtable.sel=taxtable[taxtable[,x]==paste0("t",query_taxline[x]),]
taxtable.sel=taxtable.sel[rownames(taxtable.sel)!=opt[["querytax"]],]
taxtable.sel=taxtable.sel[as.numeric(taxtable.sel[,"NumProts"])>=giveback[["NumProts"]]/2,]

giveback[["NumFriends"]]=nrow(taxtable.sel)
giveback[["NumNoData"]]=sum(ctgDB[,"ProtSupport"]==333333)
giveback[["medRLE"]]=RLEmed.s[[x]]

giveback[["medRLE_no_null"]]=RLEmed_no_null.s[[x]]
giveback[["Indiv_ProtDrop"]]=length(indiv_protdrop.x)
giveback[["Contig_ProtDrop"]]=length(ctgdrop.x[,"ProteinID"])
giveback[["IndivNTagsKept"]]=length(unique(tagsummary[["indiv_prot_kept"]]))
giveback[["IndivNTagsDropped"]]=length(unique(tagsummary[["indiv_prot_dropped"]]))
giveback[["ContigTagsKept"]]=length(unique(tagsummary[["ctg_prot_kept"]]))
giveback[["ContigTagsDropped"]]=length(unique(tagsummary[["ctg_prot_dropped"]]))
giveback[["NMixedTags"]]=nrow(tagsummary[["tagmatrix"]])
if(giveback[["NMixedTags"]]>0){
giveback[["MixedTags_KeptProts"]]=sum(tagsummary[["tagmatrix"]][rownames(tagsummary[["tagmatrix"]])!=paste0("t",query_taxline[[x]]),"Kept"])
giveback[["MixedTags_DroppedProts"]]=sum(tagsummary[["tagmatrix"]][rownames(tagsummary[["tagmatrix"]])!=query_taxline[[x]],"Dropped"])
giveback[["MixedTags_KeptHostProts"]]=sum(tagsummary[["tagmatrix"]][query_taxline[[x]],"Kept"])
giveback[["MixedTags_DroppedHostProts"]]=sum(tagsummary[["tagmatrix"]][query_taxline[[x]],"Dropped"])
if(is.na(giveback[["MixedTags_KeptHostProts"]])){giveback[["MixedTags_KeptHostProts"]]=0}
if(is.na(giveback[["MixedTags_DroppedHostProts"]])){giveback[["MixedTags_DroppedHostProts"]]=0}
}else{
giveback[["MixedTags_KeptProts"]]=0
giveback[["MixedTags_DroppedProts"]]=0
giveback[["MixedTags_KeptHostProts"]]=0
giveback[["MixedTags_DroppedHostProts"]]=0
}
return(giveback)
}
res=lapply(c("family","order","class","phylum","kingdom","superkingdom"),FUN=do.stats)
names(res)=c("family","order","class","phylum","kingdom","superkingdom")

resu=list()
resu[[1]]=res
names(resu)[[1]]=opt[["querytax"]]

resu2=lapply(names(resu),function(x) {dd=resu[[x]];lapply(names(dd),function(y) {d=dd[[y]];return(c(x,y,d[["ContigTagsDropped"]],d[["ContigTagsKept"]],d[["NMixedTags"]],d[["MixedTags_DroppedProts"]],d[["MixedTags_KeptProts"]],d[["MixedTags_DroppedHostProts"]],d[["MixedTags_KeptHostProts"]],d[["Jaccard"]],d[["NumFriends"]],d[["medRLE"]],d[["medRLE_no_null"]],d[["Indiv_ProtDrop"]],d[["Contig_ProtDrop"]],d[["NumNoData"]],d[["NumProts"]]))})})
names(resu2)=names(resu)
resu2=lapply(resu2,function(x) do.call(rbind,x))
resu2=lapply(resu2,function(x)
{colnames(x)=c("QueryTaxonID","Rank","NumTagsDropByCtg","NumTagsKeptByCtg","NumMixedTags","MixedTagProtsDropByCtg","MixedTagProtsKeptByCtg","MixedTags_HostProtsDropByCtg","MixedTags_HostProtsKeptByCtg","Jaccard","NumRelSP","MedRLE","medRLE_no_null","IndivProtDrop","CtgProtDrop","NumNoData","NumProts");return(x)})

resu3=do.call(rbind,resu2)
options(width=200)

resu3[,"QueryTaxonID"]=gsub("^t","",query_taxline)
resu3=cbind(taxDict[query_taxline],resu3)
colnames(resu3)[1]="QueryTaxonName"
resu3=data.frame(resu3,stringsAsFactors=F)
diagxls_out=paste0(outdir,"/diag_data/",opt[["what"]],"_RunDiag.xlsx")
WriteXLS(list("RunDiag"=resu3),diagxls_out)

#At each taxon rank, we callect all dropped proteins / contigs up in the taxon lineage (superkingdom, superkingdom+kingdom, superkingdom+kingdom+phylum)
cumulative_ctg_drop=lapply(1:length(offsets),function(x) return(unique(unlist(drop_contigs[x:length(offsets)]))))
cumulative_prot_drop=lapply(cumulative_ctg_drop,function(x) return(ctgDB[ctgDB[,"Contig"]%in%x,]))
names(cumulative_ctg_drop)=names(cumulative_prot_drop)=names(offsets)

library(Biostrings)
protDB=readAAStringSet(protfile)
protDB.TrimNames=gsub(" .+","",names(protDB))

saveRDS(cumulative_ctg_drop,paste0(outdir,"/R_saved_objects/cumulative_ctg_drop.RDS"))
saveRDS(cumulative_prot_drop,paste0(outdir,"/R_saved_objects/cumulative_prot_drop.RDS"))


fa_exporter=function(x)
{
#This function creates the output folders (one per taxon rank) and saves unfiltered / filtered fasta files there. 
info(paste0("Exporting filtered data, processed at ",'"',x,'"', " level."))
outsubdir=paste0(opt[["outdir"]],"/filtered_outputs/",x,"_level/")
dir.create(outsubdir)
dropped_prot_taxdata=cumulative_prot_drop[[x]]
dropped_ctg_taxdata=cumulative_ctg_drop[[x]]
if(opt[["genome_filter"]])
{
DNA=readDNAStringSet(dnafile)
GFF=readGFF(annotfile)
dna.kept=DNA[!names(DNA)%in%dropped_ctg_taxdata]
dna.dropped=DNA[names(DNA)%in%dropped_ctg_taxdata]
GFF.kept=GFF[!GFF[,1]%in%dropped_ctg_taxdata,]
GFF.dropped=GFF[GFF[,1]%in%dropped_ctg_taxdata,]
writeXStringSet(dna.kept,paste0(outsubdir,opt[["what"]],"_kept_dna.fasta"))
if(length(dna.dropped)>0)
{
writeXStringSet(dna.dropped,paste0(outsubdir,opt[["what"]],"_dropped_dna.fasta"))
}else
{
writeLines("",paste0(outsubdir,opt[["what"]],"_dropped_dna.fasta"))
}
export(GFF.kept,paste0(outsubdir,opt[["what"]],"_kept_annot.gff3"),format='gff3')
if(nrow(GFF.dropped)>0)
{
export(GFF.dropped,paste0(outsubdir,opt[["what"]],"_dropped_annot.gff3"),format='gff3')
}else
{
writeLines("",paste0(outsubdir,opt[["what"]],"_dropped_annot.gff3"))
}
}
for(i in colnames(dropped_prot_taxdata)){
dropped_prot_taxdata[,i]=gsub("^t","",dropped_prot_taxdata[,i])
}
dpt_excelfile=paste0(outsubdir,x,"_dropped_protein_taxontable.xlsx")
dpt_outdata=list()
dpt_outdata[[paste0(x,"_dropped_prot")]]=dropped_prot_taxdata
WriteXLS(dpt_outdata,dpt_excelfile)
kept=protDB[!protDB.TrimNames%in%dropped_prot_taxdata[,"ProteinID"]]
dropped=protDB[protDB.TrimNames%in%dropped_prot_taxdata[,"ProteinID"]]
writeXStringSet(kept,paste0(outsubdir,opt[["what"]],"_kept_proteins.fasta"))
writeXStringSet(dropped,paste0(outsubdir,opt[["what"]],"_dropped_proteins.fasta"))
message=c(paste0("Processed ",length(protDB)," proteins."),paste0("Marked for removal: ",nrow(dropped_prot_taxdata)),paste0("Found and removed: ",length(dropped)),paste0("Kept: ",length(kept)))
info(message)

if(nrow(dropped_prot_taxdata)/length(protDB)>0.95)
{
info(paste0("Warning, more than 95% the proteins were marked as contamination at ",'"',x,'"', " level.\nPlease double-check that the NCBI taxon ID was properly set for the host."))
}else if(nrow(dropped_prot_taxdata)/length(protDB)>=0.5)
{
info(paste0("Warning, more than half of the proteins were marked as contamination at ",'"',x,'"', " level."))
}
return(0)
}
res=lapply(names(cumulative_ctg_drop),FUN=fa_exporter)


cat("ContScout finished.\n")
#EOF#
