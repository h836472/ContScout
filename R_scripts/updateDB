#!/usr/bin/Rscript
###########################
#library and arguments init
###########################
startdir=getwd()
time_start=Sys.time()
logfilename=paste0("ContScout_Updater_",format(time_start,"%d%b_%Y_%H_%M"),".log")
logfile=file(logfilename,open="a") 
progname="updateDB"
cat("This is the database updater component of ContScout.\n")
cat("Loading R libraries.\n") 
suppressPackageStartupMessages(library("optparse"))

suppressPackageStartupMessages(library(parallel))
suppressPackageStartupMessages(library(rjson))
suppressPackageStartupMessages(library(XML))
suppressPackageStartupMessages(library(googlesheets4))
suppressPackageStartupMessages(library("GenomicRanges"))
cargs=commandArgs(trailingOnly=T)
option_list = list(
  make_option(c("-u", "--userdir"), type="character", default=NULL, 
              help="Path to local database repository folder", metavar="userdir"),
  make_option(c("-i", "--info"), type="character", default="https://docs.google.com/spreadsheets/d/1_FPaAnyHVUhHzV8gwic2X3RKaN9vSl9jcTN_50T7X24", 
              help="Path to local database repository folder", metavar="info"),
  make_option(c("-l", "--list_databases"), action="store_true", default=FALSE,
              help="List pre-formatted databases stored in the local database repository."),
  make_option(c("-c", "--cpu"), type="character", default="all", 
              help="Number of CPUs to use", metavar="cpu"),
  make_option(c("-d", "--dbname"), type="character", default=NULL,help="Name of the database to be used as a reference",metavar="dbname"));
opt_parser = OptionParser(option_list=option_list,prog=progname);

if(FALSE)
{
#manually set options, if you run the code interactively, for debugging/development
opt=list()
opt[["dbname"]]="swissprot" #-d
opt[["cpu"]]=24 #-c 
opt[["info"]]="https://docs.google.com/spreadsheets/d/1_FPaAnyHVUhHzV8gwic2X3RKaN9vSl9jcTN_50T7X24"
opt[["userdir"]]="/work/balintb/databases_dev"

}else{
opt = parse_args(opt_parser);
}
###################################
#generic logging and info functions
###################################
byedie=function(x,log=TRUE,sure_q=TRUE){
cat(x)
if(log)
 {
 writeLines(x,con=logfile)
 }
if(sure_q)
 {
 q(save="no",status=1)
 }else
 {
 cat("\nFatal error found but quit cancelled by sure_q=FALSE...\n")
 }
}

#user dir check. Reference databases are kept in this directory, created by updateDB
if(is.null(opt[["userdir"]]))
{
byedie(paste0("Please provide the path to your user directory (-u) containing the pre-formatted reference databases.\nIf you do not have any local database yet, you can use the tool \'updateDB\'.","\n"))
}

#detect reference databases
databases=list.files(opt[["userdir"]],pattern="\\.dbinfo",recursive=T,full.names=T)
names(databases)=gsub("\\.dbinfo$","",gsub("^.+/","",databases))
names(databases)=gsub("_",":",names(databases))
dbdata=lapply(databases,function(x) readLines(x))

#if -l switch is present, tool lists all configured reference databases and quits

if (opt[["list_databases"]])
{
cat("Listing pre-formatted reference databases.\n")
res=lapply(names(dbdata),function(x) writeLines(c(paste0("### ",x," ###"),dbdata[[x]],"\n")))
cat(paste0("When running ContScout, please select a database. Examples: ",'"',"-d refseq",'"',", ",'"',"-d refseq:latest",'"',",",'"',"-d refseq:04f69a06",'"',"\n"))
q(save="no",status=0)
}




if(is.null(opt[["dbname"]])){
byedie("Please specify a database to be downloaded. Example: -d swissprot")
}


info=function(x,log=F){
cat(paste(x,"\n"))
if(log)
 {
 writeLines(x,con=logfile)
 }
}

jacksum=function(x)
{
crc=system(paste0("jacksum -a crc32 -E hex ",x),intern=T)
return(gsub("\\s.+$","",crc[length(crc)]))
#issue with container: jacksum output might contain an surplus line with a weird warning, poisoning checksum. Why not? Be my guest!
}
create.dir=function(x){
#folder creation with error checking
dir.create(x)
return(dir.exists(x))
}

user.input = function(prompt) {
  if (interactive()) {
    return(readline(prompt))
  } else {
    cat(prompt)
    return(readLines("stdin", n=1))
  }
}


if(is.null(opt[["userdir"]]))
{
stop("Please set your local database directory with -u or --userdir option!")
}


info("Downloading database URL file")

if(grepl("^http",opt[["info"]])){
#default variable is kept: download "official" db register from GoogleDrive
gs4_deauth()
DBinfo=try(read_sheet(opt[["info"]]),silent=T)
if(!"data.frame"%in%class(DBinfo))
 {
 byedie("Unable to download database URL file from Google Cloud.")
 } 
}else
{
#variable is set: local file is expected
DBinfo=read.table(opt[["info"]],sep=",",as.is=T,row.name=1,header=T)
}
DBinfo=data.frame(DBinfo,stringsAsFactors=F)
rownames(DBinfo)=DBinfo[,"DB_Name"]

if((!grepl("^custom:.+:/.+$",opt[["dbname"]]))&& (!("ncbi_taxonomy"%in%rownames(DBinfo) && opt[["dbname"]]%in% rownames(DBinfo))))
{
byedie(paste0("Database \'",opt[["dbname"]],"\' is not recognized. Please consult the manual for possible fixes of this error!\n"))
}


reformat_DB=function(x)
{
cpu=grep("Flags:",system("lscpu",intern=T),value=T)
if(grepl(" avx2 ",cpu)){
mmseqs.bin="mmseqs_avx2"
} else if (grepl(" sse4_1 ",cpu)) {
mmseqs.bin="mmseqs_sse41"
} else {
mmseqs.bin="mmseqs_sse2"
}
x.s=unlist(strsplit(x,"/"))
outfolder=paste(x.s[-length(x.s)],collapse="/")
outfile=x.s[length(x.s)]
outdb_name=gsub("\\.fasta$","",outfile)
dir.create(paste0(outfolder,"/mmseqs"))
mmseqs.cmd=paste0(mmseqs.bin," createdb ",x," ",paste0(outfolder,"/mmseqs/",outdb_name,".taxdb"))
system(mmseqs.cmd)

if(!file.exists(paste0(outfolder,"/mmseqs/",outdb_name,".taxdb"))&& file.info(paste0(outfolder,"/mmseqs/",outdb_name,".taxdb"))$size>0)
{
byedie("MMSeqs database generation failed!\n")
}

dir.create(paste0(outfolder,"/diamond"))
diamond.cmd=paste0("diamond makedb --in ",x," -d ",paste0(outfolder,"/diamond/",outdb_name,".taxdb"))
system(diamond.cmd)

if(!file.exists(paste0(outfolder,"/diamond/",outdb_name,".taxdb.dmnd"))&& file.info(paste0(outfolder,"/diamond/",outdb_name,".taxdb.dmnd"))$size>0)
{
byedie("Diamond database generation failed!\n")
}

return(c(paste0(outfolder,"/mmseqs/",outdb_name,".taxdb"),paste0(outfolder,"/diamond/",outdb_name,".taxdb.dmnd")))
}

count_TaxIDs=function(x)
{
#calculate number of taxa in reference database
x.s=unlist(strsplit(x,"/"))
taxi=system(paste('grep ">" ',x,' | cut -f 1 -d " " | cut -f 2 -d :'),intern=T)
workdir=paste(x.s[-length(x.s)],collapse="/")
r=Rle(taxi)
rundata=cbind(runValue(r),runLength(r))
colnames(rundata)=c("TaxID","RunLength")
rundata.s=split(rundata[,"RunLength"],rundata[,"TaxID"])
rundata.s2=sapply(rundata.s,function(x) return(sum(as.numeric(x))))
rundata.s3=rundata.s2[sapply(rundata.s2,function(x) x>=100)] #remove taxon IDs with fewer than 100 proteins
taxfile=list.files(workdir,pattern="rankedtaxidlineage.RDS",full.names=T,recursive=T)
tax=readRDS(taxfile)
summary(names(rundata.s3)%in%rownames(tax))
s=cbind(rundata.s3,names(rundata.s3))
colnames(s)=c("NumProts","TaxonID")
s=s[s[,"TaxonID"]%in%rownames(tax),]
rownames(s)=s[,"TaxonID"]
tax.sel=tax[rownames(tax)%in%rownames(s),]
tax.sel=tax.sel[rownames(s),]
all(rownames(tax.sel)==rownames(s))
taxtable=cbind(tax.sel,s)
taxtable=data.frame(taxtable,stringsAsFactors=F)
for(colname in c("family","order","class","phylum","kingdom","superkingdom"))
{taxtable[,colname]=paste0("t",taxtable[,colname])
}
saveRDS(taxtable,paste0(workdir,"/",x.s[[2]],"_",x.s[[1]],"_taxID_DBcount.RDS"))
return(0)
}


#####################################################
#Database-specific downloader scripts are here
#####################################################

update_is_needed=function(db,crc,udir)
{
ifile=list.files(udir,pattern="dbinfo",recursive=T,full.name=T)
ifile=grep(db,ifile,value=T)
ifile=grep(crc,ifile,value=T)
if(length(ifile)==0)
{
return(TRUE)
}else{
return(FALSE)
}
}

dl.ncbi_tax=function(x)
{
x.s=unlist(strsplit(x,"/"))
workdir=paste(x.s[-length(x.s)],collapse="/")
outdir=(paste0(workdir,"/ncbi_tax"))
create.dir(outdir)
cat("Fetching NCBI taxonomy database.\n")
timestamp=paste0("ncbi_taxonomy","_",format(Sys.time(),"%Y%m%d%H%M"))
dir.create(paste0("tmp/",timestamp))
md5.download.cmd=paste0("wget --no-check-certificate ",DBinfo["ncbi_taxonomy","DB_URL"]," -O tmp/",timestamp,"/new_taxdump.tar.gz.md5")
db.download.cmd=gsub("new_taxdump.tar.gz.md5","new_taxdump.tar.gz",md5.download.cmd)
system(md5.download.cmd)
system(db.download.cmd)
taxdump.md5=substr(system(paste0("md5sum tmp/",timestamp,"/new_taxdump.tar.gz"),intern=T),1,32)
taxdump.refmd5=substr(readLines(paste0("tmp/",timestamp,"/new_taxdump.tar.gz.md5")),1,32)
info.md5=substr(system(paste0("md5sum tmp/",timestamp,"/new_taxdump.tar.gz.md5"),intern=T),1,32)
if(taxdump.refmd5!=taxdump.md5)
{
byedie(paste0("NCBI taxonomy database failed to download!\nDownload from NCBI failed!"))
}else 
{
system(paste0("tar -xvf tmp/",timestamp,"/new_taxdump.tar.gz -C tmp/",timestamp))
taxdump.crc32=jacksum(paste0("tmp/",timestamp,"/rankedlineage.dmp"))
dir.create(paste0(outdir,"/",taxdump.crc32))
file.rename(paste0("tmp/",timestamp,"/rankedlineage.dmp"),paste0(outdir,"/",taxdump.crc32,"/",taxdump.crc32,"_rankedlineage.dmp"))
file.rename(paste0("tmp/",timestamp,"/nodes.dmp"),paste0(outdir,"/",taxdump.crc32,"/",taxdump.crc32,"_nodes.dmp"))
file.rename(paste0("tmp/",timestamp,"/taxidlineage.dmp"),paste0(outdir,"/",taxdump.crc32,"/",taxdump.crc32,"_taxidlineage.dmp"))
info(paste0("Generating ranked taxID file from NCBI database. This might take a while."))
nodedata=readLines(paste0(outdir,"/",taxdump.crc32,"/",taxdump.crc32,"_nodes.dmp"))
lineage=readLines(paste0(outdir,"/",taxdump.crc32,"/",taxdump.crc32,"_taxidlineage.dmp"))

nodedata.list=mclapply(nodedata,function(x) unlist(strsplit(x,"\t|\t",fixed=T)),mc.cores=opt[["cpu"]])
nodedata.ranks=sapply(nodedata.list,function(x) return(x[3]))
ranks.sel=tolower(c("Genus","Family","Order","Class","Phylum","Kingdom","Superkingdom"))
nodedata.list.sel=nodedata.list[nodedata.ranks%in%ranks.sel] 
node_whitelist=lapply(nodedata.list.sel,function(x) x[c(1,3)]) 
node_whitelist=do.call(rbind,node_whitelist)
colnames(node_whitelist)=c("TaxID","Rank")
node_whitelist.s=split(node_whitelist[,"TaxID"],node_whitelist[,"Rank"])

lineage.list=mclapply(lineage,function(x) unlist(strsplit(x,"\t|\t",fixed=T)),mc.cores=opt[["cpu"]])
names(lineage.list)=sapply(lineage.list,function(x) paste0("t",x[1]))
lineage.list=sapply(lineage.list,function(x) return(gsub("\t|","",x[2],fixed=T)))

lineage.LL=mclapply(lineage.list,function(x) unlist(strsplit(x," ")),mc.cores=opt[["cpu"]])
names(lineage.LL)=paste0(names(lineage.LL),":")
lineage.ul=unlist(lineage.LL)

for(rank in ranks.sel)
{
lineage.ul[lineage.ul%in%node_whitelist.s[[rank]]]=paste0(rank,":",lineage.ul[lineage.ul%in%node_whitelist.s[[rank]]])
}

lineage.ul=grep(":",lineage.ul,value=T)
names(lineage.ul)=gsub(":.*","",names(lineage.ul))
lineage.s=split(lineage.ul,names(lineage.ul))

extend=function(x)
{
names(x)=gsub(":.+$","",x)
miss_tax=setdiff(ranks.sel,names(x))
res=unlist(lapply(miss_tax,function(x) {a="";names(a)=x;return(a)}))
x=c(res,x)
x=x[tolower(c("Genus","Family","Order","Class","Phylum","Kingdom","Superkingdom"))]
x=gsub("^.+:","",x)
return(x)
}

lineage.es=mclapply(lineage.s,FUN=extend,mc.cores=opt[["cpu"]])
lineage.es=do.call(rbind,lineage.es)

lineage.es[lineage.es[,"genus"]=="","genus"]=32644
lineage.es[lineage.es[,"family"]=="","family"]=paste0("vf_",lineage.es[lineage.es[,"family"]=="","genus"])
lineage.es[lineage.es[,"order"]=="","order"]=paste0("vo_",lineage.es[lineage.es[,"order"]=="","family"])
lineage.es[lineage.es[,"class"]=="","class"]=paste0("vc_",lineage.es[lineage.es[,"class"]=="","order"])
lineage.es[lineage.es[,"phylum"]=="","phylum"]=paste0("vp_",lineage.es[lineage.es[,"phylum"]=="","class"])
lineage.es[lineage.es[,"kingdom"]=="","kingdom"]=paste0("vk_",lineage.es[lineage.es[,"kingdom"]=="","phylum"])
lineage.es=lineage.es[lineage.es[,"superkingdom"]!="",]
lineagefile=paste0(outdir,"/",taxdump.crc32,"/",taxdump.crc32,"_rankedtaxidlineage.RDS")
saveRDS(lineage.es,lineagefile)
unlink(paste0("tmp/",timestamp),recursive=T)
return(taxdump.crc32)
}
}

dl.ncbi_blastdb=function(wd=opt[["userdir"]],sd=startdir,what,i=DBinfo)
{
setwd(wd)
if(!dir.exists(what))
{
create.dir(what)
}
info(paste0("Fetching BLAST database \'",what,"\' from NCBI.\n"))
timestamp=paste0(what,"_",format(Sys.time(),"%Y%m%d%H%M"))
create.dir(paste0("tmp/",timestamp))
info_outfile=gsub("^.+/","",i[what,"DB_URL"])
info.download.cmd=paste0("wget --no-check-certificate  ",i[what,"DB_URL"]," -O tmp/",timestamp,"/",info_outfile)
system(info.download.cmd)
info.md5=substr(system(paste0("md5sum tmp/",timestamp,"/",info_outfile),intern=T) ,1,32)
info.crc=jacksum(paste0("tmp/",timestamp,"/",info_outfile))

if(update_is_needed(db=what,crc=info.crc,udir=opt[["userdir"]]))
{
info.data=fromJSON(file=paste0("tmp/",timestamp,"/",info_outfile))
dl.chunks=matrix(ncol=4,nrow=length(info.data$files))
dl.chunks[,1]=paste0("wget -c --no-check-certificate ",gsub(paste0(info_outfile,"$"),"",i[what,"DB_URL"]))
dl.chunks[,2]=gsub("^.+/","",info.data$files)
dl.chunks[,3]=paste0(" -O tmp/",timestamp,"/")
dl.chunks[,4]=gsub("^.+/","",info.data$files)
dl.commands=apply(dl.chunks,1,function(x) paste(x,collapse=""))
dl.md5.commands=gsub("\\.tar\\.gz",".tar.gz.md5",dl.commands)
dl.check.md5=lapply(dl.md5.commands,function(x) return(system(x)))

if(!all(unlist(dl.check.md5)==0))
{
byedie(paste0("Download of database \'",what,"\' failed!","\n"))
}
dl.check=lapply(dl.commands,function(x) return(system(x)))
if(!all(unlist(dl.check)==0))
{
byedie(paste0("Download of database \'",what,"\' failed!","\n"))
}
gzf=list.files(pattern="gz$",paste0("tmp/",timestamp),full.name=T)
mdf=list.files(pattern="gz.md5",paste0("tmp/",timestamp),full.name=T)
mdd=lapply(mdf,function(x) readLines(x))
names(mdd)=gsub("\\.md5$","",gsub("^.+/","",mdf))
md5sums.exp=sapply(mdd,function(x) substr(x,1,32))
#check chunk md5sums here
md5sums.obs=mclapply(gzf,function(x) system(paste0("md5sum ",x),intern=T),mc.cores=opt[["IO_cpu"]])
names(md5sums.obs)=gsub("^.+/","",gzf)
md5sums.obs=sapply(md5sums.obs,function(x) substr(x,1,32))
if(!(length(md5sums.obs)==length(md5sums.exp) && all(names(md5sums.exp)==names(md5sums.obs))&& all(md5sums.exp==md5sums.obs)))
{
byedie(paste0("MD5 checksum mismatch, database download error with \'",what,"\'!"))
}
gzf=gzf[length(gzf):1]
lapply(gzf,function(x) system(paste0("tar -xvf ",x," -C tmp/",timestamp)))
setwd(paste0("tmp/",timestamp))
system(paste0('blastdbcmd -dbtype prot -db ',what,' -entry all -outfmt ">%a\t%T\t%t\t%s" >',timestamp,"_dump.fasta")) 
#watch out! \t is recognized while \n is NOT.

num_prot_out=system(paste0('grep -c ">" ',timestamp,"_dump.fasta"),intern=T)
setwd(opt[["userdir"]])
fasta_con=file(paste0("tmp/",timestamp,"/",timestamp,"_dump.fasta"))
open(fasta_con,open="r")
while(TRUE)
{
start=Sys.time()
chunk=scan(fasta_con,n=2000000,what="char",sep="\t",quote="") 
if(length(chunk)==0)
{
break
}else
{
chunk=matrix(chunk,byrow=T,ncol=4)
nprot=nrow(chunk)
chunk[,2]=paste0("t",chunk[,2])
new.header=paste0(chunk[,1],":",chunk[,2]," ",chunk[,3])
new.fasta=c(rbind(new.header,chunk[,4]))
write.table(new.fasta,paste0("tmp/",timestamp,"/",timestamp,"_tax.fasta"),append=T,quote=F,row.names=F,col.names=F)
stop=Sys.time()
cat (paste0("Processed ",nprot," proteins.\n"))
print(stop-start)
}
}
close(fasta_con)
fasta.crc=jacksum(paste0("tmp/",timestamp,"/",timestamp,"_tax.fasta"))
dir.create(paste0(what,"/",info.crc))
dir.create(paste0(what,"/",info.crc,"/db_info"))
fa_outfile=paste0(what,"/",info.crc,"/",fasta.crc,"_",what,"_tax.fasta")
file.rename(paste0("tmp/",timestamp,"/",timestamp,"_tax.fasta"),fa_outfile)
numheaders=system(paste0('grep -c ">" ',fa_outfile),intern=T)
info(paste0("Written ",numheaders," sequences with TaxonID."))
timetext=format(Sys.time(), "%Y-%m-%d_%H:%M:%S")
db_loc=reformat_DB(fa_outfile)
tax_CRC=dl.ncbi_tax(fa_outfile)
count_TaxIDs(fa_outfile)
json_outfile=paste0(what,"/",info.crc,"/db_info/",info.crc,"_",info_outfile)
file.rename(paste0("tmp/",timestamp,"/",info_outfile),json_outfile)
unlink(paste0("tmp/",timestamp),recursive=T)
db_summary=paste0(c("Name: ","Source: ","NumProts: ","DB_CRC: ","Tax_CRC: ","MMSeqs_DB: ","Diamond_DB: ","Creation_Date: "),c(what,"NCBI",numheaders,fasta.crc,tax_CRC,db_loc,timetext))
writeLines(db_summary,paste0(what,"/",info.crc,"/db_info/",what,"_",info.crc,".dbinfo"))
unlink(paste0("tmp/",timestamp),recursive=T)
system(paste0("pigz -p ",opt[["cpu"]]," ",fa_outfile))
}else
{
info(paste0("Latest version of database ",'"',what,'"'," is already installed."))
}
}

fetch.uniprot=function(x)
{
system(x[["command"]])
data.outfile=gsub("^.+ -O tmp","tmp",x[["command"]])
data.md5.obs=substr(system(paste0("md5sum ",data.outfile),intern=T),1,32)
if(!file.info(data.outfile)$size!=x[["size"]] && data.md5.obs!=x[["md5"]])
{
byedie(paste0("File size or MD5 signature mismatch detected!\nDownload failed for database \'",what,"\'!"))
}else{return(data.outfile)}
}

dl.uniprot=function(wd=opt[["userdir"]],sd=startdir,i=DBinfo,what)
{
setwd(wd)
if(!dir.exists(what))
{
create.dir(what)
}
info(paste0("Fetching database \'",what,"\' from Expasy."))
timestamp=paste0(what,"_",format(Sys.time(),"%Y%m%d%H%M"))
create.dir(paste0("tmp/",timestamp))
info_outfile=gsub("^.+/","",i[what,"DB_URL"])
info.download.cmd=paste0("wget --no-check-certificate  ",i[what,"DB_URL"]," -O tmp/",timestamp,"/",info_outfile)
system(info.download.cmd)
info.md5=substr(system(paste0("md5sum tmp/",timestamp,"/",info_outfile),intern=T) ,1,32)
info.crc=jacksum(paste0("tmp/",timestamp,"/",info_outfile))
if(update_is_needed(db=what,crc=info.crc,udir=opt[["userdir"]]))
{
info.data=(xmlParse(paste0("tmp/",timestamp,"/",info_outfile)))
info.data=do.call(paste, as.list(capture.output(info.data)))
if(what==("uniref90")
{
info.data.decode=list()
info.data.decode[["uniref90.fasta.gz"]]=list()
info.data.decode[["uniref90.fasta.gz"]][["version"]]=gsub("^.+<version>","",gsub("</version>.+$","",info.data))
info.data.decode[["uniref90.fasta.gz"]][["md5"]]=gsub("^.+>","",gsub("</hash>.+$","",gsub("^.+<file name=\"uniref90.fasta.gz\">","",info.data)))
info.data.decode[["uniref90.fasta.gz"]][["size"]]=gsub("^.+>","",gsub("</size>.+$","",gsub("^.+<file name=\"uniref90.fasta.gz\">","",info.data)))
info.data.decode[["uniref90.fasta.gz"]][["command"]]=gsub(info_outfile,"uniref90.fasta.gz",info.download.cmd)
}
else if(what=="uniref100")
{
info.data.decode=list()
info.data.decode[["uniref100.fasta.gz"]]=list()
info.data.decode[["uniref100.fasta.gz"]][["version"]]=gsub("^.+<version>","",gsub("</version>.+$","",info.data))
info.data.decode[["uniref100.fasta.gz"]][["md5"]]=gsub("^.+>","",gsub("</hash>.+$","",gsub("^.+<file name=\"uniref100.fasta.gz\">","",info.data)))
info.data.decode[["uniref100.fasta.gz"]][["size"]]=gsub("^.+>","",gsub("</size>.+$","",gsub("^.+<file name=\"uniref100.fasta.gz\">","",info.data)))
info.data.decode[["uniref100.fasta.gz"]][["command"]]=gsub(info_outfile,"uniref100.fasta.gz",info.download.cmd)
}
else if (what=="uniprotKB")
{
info.data.decode=list()
info.data.decode[["uniprot_sprot.fasta.gz"]]=list()
info.data.decode[["uniprot_sprot.fasta.gz"]][["version"]]=gsub("^.+<version>","",gsub("</version>.+$","",info.data))
info.data.decode[["uniprot_sprot.fasta.gz"]][["md5"]]=gsub("^.+>","",gsub("</hash>.+$","",gsub("^.+<file name=\"uniprot_sprot.fasta.gz\">","",info.data)))
info.data.decode[["uniprot_sprot.fasta.gz"]][["size"]]=gsub("^.+>","",gsub("</size>.+$","",gsub("^.+<file name=\"uniprot_sprot.fasta.gz\">","",info.data)))
info.data.decode[["uniprot_sprot.fasta.gz"]][["command"]]=gsub(info_outfile,"uniprot_sprot.fasta.gz",info.download.cmd)
info.data.decode[["uniprot_trembl.fasta.gz"]]=list()
info.data.decode[["uniprot_trembl.fasta.gz"]][["version"]]=gsub("^.+<version>","",gsub("</version>.+$","",info.data))
info.data.decode[["uniprot_trembl.fasta.gz"]][["md5"]]=gsub("^.+>","",gsub("</hash>.+$","",gsub("^.+<file name=\"uniprot_trembl.fasta.gz\">","",info.data)))
info.data.decode[["uniprot_trembl.fasta.gz"]][["size"]]=gsub("^.+>","",gsub("</size>.+$","",gsub("^.+<file name=\"uniprot_trembl.fasta.gz\">","",info.data)))
info.data.decode[["uniprot_trembl.fasta.gz"]][["command"]]=gsub(info_outfile,"uniprot_trembl.fasta.gz",info.download.cmd)
}else if (what=="uniprotSP")
{
#this is for development only, downloading only the swiss prot part of uniprotKB
info.data.decode=list()
info.data.decode[["uniprot_sprot.fasta.gz"]]=list()
info.data.decode[["uniprot_sprot.fasta.gz"]][["version"]]=gsub("^.+<version>","",gsub("</version>.+$","",info.data))
info.data.decode[["uniprot_sprot.fasta.gz"]][["md5"]]=gsub("^.+>","",gsub("</hash>.+$","",gsub("^.+<file name=\"uniprot_sprot.fasta.gz\">","",info.data)))
info.data.decode[["uniprot_sprot.fasta.gz"]][["size"]]=gsub("^.+>","",gsub("</size>.+$","",gsub("^.+<file name=\"uniprot_sprot.fasta.gz\">","",info.data)))
info.data.decode[["uniprot_sprot.fasta.gz"]][["command"]]=gsub(info_outfile,"uniprot_sprot.fasta.gz",info.download.cmd)
}
dl_outputs=sapply(info.data.decode,FUN=fetch.uniprot)
out.fasta.file=paste0("tmp/",timestamp,"/",what,"_",info.data.decode[[1]][["version"]],".fasta")
gunzip.cmd=paste0("zcat ",paste(dl_outputs,collapse=" ")," >",out.fasta.file )

gunzip.res=system(gunzip.cmd)
if(!gunzip.res==0)
{
byedie(paste0("Database decompression failed for \'",what,"'\"."))
}
uni_con=file(out.fasta.file)
open(uni_con,open="r")
while(TRUE)
{
start=Sys.time()
chunk=readLines(uni_con,n=50000000) 
if(length(chunk)==0)
{
break
}else
{
fchar=substr(chunk,1,1)
headers=fchar==">"
nprot=sum(headers)
chunk.taxid=gsub("^.+( TaxID=| OX=)","t",chunk[headers])
chunk.taxid[chunk.taxid=="tN/A"]="t32644"
chunk.taxid=gsub(" .+","",chunk.taxid)
chunk.id=gsub(" .+$","",chunk[headers])
chunk.comment=gsub("^\\S+ ","",chunk[headers])
newheaders=paste0(chunk.id,":",chunk.taxid," ",chunk.comment)
newheaders=gsub("\\|","_",newheaders)
chunk[headers]=newheaders
write.table(chunk,paste0("tmp/",timestamp,"/",what,"_",info.data.decode[[1]][["version"]],"_tax.tmp"),append=T,quote=F,row.names=F,col.names=F)
stop=Sys.time()
cat (paste0("Processed ",nprot," proteins.\n"))
print(stop-start)
}
}                  
close(uni_con)
fasta.crc=jacksum(paste0("tmp/",timestamp,"/",what,"_",info.data.decode[[1]][["version"]],"_tax.tmp"))
fasta.crc=gsub("\\s.+$","",fasta.crc)
dir.create(paste0(what,"/",info.crc))
dir.create(paste0(what,"/",info.crc,"/db_info"))
fa_outfile=paste0(what,"/",info.crc,"/",fasta.crc,"_",what,"_",info.data.decode[[1]][["version"]],"_tax.fasta")
file.rename(paste0("tmp/",timestamp,"/",what,"_",info.data.decode[[1]][["version"]],"_tax.tmp"),fa_outfile)
numheaders=system(paste0('grep -c ">" ',fa_outfile),intern=T)
info(paste0("Written ",numheaders," sequences with TaxonID."))
timetext=format(Sys.time(), "%Y-%m-%d_%H:%M:%S")
db_loc=reformat_DB(fa_outfile)
tax_CRC=dl.ncbi_tax(fa_outfile)
count_TaxIDs(fa_outfile)
json_outfile=paste0(what,"/",info.crc,"/db_info/",info.crc,"_",info_outfile)
file.rename(paste0("tmp/",timestamp,"/",info_outfile),json_outfile)

db_summary=paste0(c("Name: ","Source: ","NumProts: ","DB_CRC: ","Tax_CRC: ","MMSeqs_DB: ","Diamond_DB: ","Creation_Date: "),c(what,"Uniprot",numheaders,fasta.crc,tax_CRC,db_loc,timetext))
writeLines(db_summary,paste0(what,"/",info.crc,"/db_info/",what,"_",info.crc,".dbinfo"))
unlink(paste0("tmp/",timestamp),recursive=T)
system(paste("gzip ",fa_outfile))
}else
{
info(paste0("Latest version of database ",'"',what,'"'," is already installed."))
}
}

import_custom_set=function(x)
{
x.s=unlist(strsplit(x,":"))
fasta.file=list.files(pattern="fasta$",x.s[[3]],full.name=T)
taxid.file=gsub("\\.fasta$",".tax",fasta.file)
if(!file.exists(fasta.file)&&file.exists(taxid.file))
{
byedie(paste0("Could not find .fasta and .TaxID file in custom datatabase import folder ",'"',x.s[[3]],'".'))
}
numprot=system(paste0('grep -c ">" ',fasta.file),intern=T)
numtax=system(paste0('egrep -c -v "^$" ',taxid.file),intern=T)
if(!numprot==numtax)
{
byedie("Number of proteins and number of taxonIDs do not match. Please check input files!\n")
}
fasta_con=file(fasta.file)
tax_con=file(taxid.file)
open(fasta_con,open="r")
open(tax_con,open="r")
timestamp=paste0(x.s[[2]],"_",format(Sys.time(),"%Y%m%d%H%M"))
while(TRUE)
{
start=Sys.time()
chunk=scan(fasta_con,n=2000000,what="char",sep="\t",quote="") 
if(length(chunk)==0)
{
break
}else
{
fchar=substr(chunk,1,1)
headers=fchar==">"
nprot=sum(headers)
taxchunk=scan(tax_con,n=nprot,what="char",sep="\t",quote="") 
taxchunk=grep("^$",taxchunk,invert=T,value=T)

old_names=chunk[headers]
taxchunk=gsub("NA","t32644",taxchunk)
old_names.t=do.call(rbind,lapply(old_names,function(x) {d=unlist(strsplit(x," "));d_rest=paste(d[-1],collapse=" ");return(c(d[1],d_rest))}))
new_names=paste0(old_names.t[,1],":t",taxchunk," ",old_names.t[,2])
new_names=gsub("\\|","_",new_names)
chunk[headers]=new_names
if(!dir.exists(paste0("tmp/",timestamp)))
{
create.dir(paste0("tmp/",timestamp))
}
write.table(chunk,paste0("tmp/",timestamp,"/",timestamp,"_customDB.fasta"),append=T,quote=F,row.names=F,col.names=F)
stop=Sys.time()
cat (paste0("Processed ",nprot," proteins.\n"))
print(stop-start)
}
}
close(fasta_con)
close(tax_con)
fasta.crc=jacksum(paste0("tmp/",timestamp,"/",timestamp,"_customDB.fasta"))

if(!dir.exists("custom"))
{
create.dir("custom")
}

dir.create(paste0("custom","/",x.s[2],"_",fasta.crc))
dir.create(paste0("custom","/",x.s[2],"_",fasta.crc,"/db_info"))
fa_outfile=paste0("custom","/",x.s[2],"_",fasta.crc,"/",x.s[2],"_",fasta.crc,"_tax.fasta")
file.rename(paste0("tmp/",timestamp,"/",timestamp,"_customDB.fasta"),fa_outfile)
numheaders=system(paste0('grep -c ">" ',fa_outfile),intern=T)
info(paste0("Written ",numheaders," sequences with TaxonID."))
timetext=format(Sys.time(), "%Y-%m-%d_%H:%M:%S")
db_loc=reformat_DB(fa_outfile)
tax_CRC=dl.ncbi_tax(fa_outfile)
count_TaxIDs(fa_outfile)
db_summary=paste0(c("Name: ","Source: ","NumProts: ","DB_CRC: ","Tax_CRC: ","MMSeqs_DB: ","Diamond_DB: ","Creation_Date: "),c(x.s[[2]],"direct_user_import",numheaders,fasta.crc,tax_CRC,db_loc,timetext))
writeLines(db_summary,paste0("custom","/",x.s[2],"_",fasta.crc,"/db_info/",x.s[2],"_",fasta.crc,".dbinfo"))
unlink(paste0("tmp/",timestamp),recursive=T)
system(paste("gzip ",fa_outfile))

}

#MAIN SCRIPT STARTS HERE



#detect num CPU, ignoring HyperThreading
numC_max=as.numeric(system("lscpu -b -p=Core,Socket | grep -v '^#' | sort -u | wc -l",intern=T)) 
#set numCPU
if(opt[["cpu"]]==0)
{
opt[["cpu"]]=numC_max
}else if(opt[["cpu"]]>numC_max){
opt[["cpu"]]=numC_max
}
opt[["IO_cpu"]]=min(opt[["cpu"]],8)
info(paste0("Using ",opt[["cpu"]]," CPUs for general tasks and ",opt[["IO_cpu"]]," CPUs for IO operations."))


########################################################################################
#check if the user database directory exists, is writeable and has at least 500 GB free space
########################################################################################
if(!dir.exists(opt[["userdir"]]))
{
	if(!create.dir(opt[["userdir"]]))
		{
		byedie(paste0("Directory ",opt[["userdir"]],",could not be created. Please check available space and user premissions.\n"))
		}
}
setwd(opt[["userdir"]])
if(!dir.exists("tmp"))
{
	if(!create.dir("tmp"))
		{
		byedie(paste0("Tmp directory could not be created. Please check available space and user premissions.\n"))
		}
}

dbd.df=system(paste0("df -P ",opt[["userdir"]]," -B 1G ",opt[["userdir"]]),intern=T)[2]
dbd.dfv=unlist(strsplit(dbd.df," +"))
dbd.free=as.numeric(dbd.dfv[[4]])
if(!dbd.free>=500)
{
byedie(paste0("There seems to be less than 500 GB free space in folder ",opt[["userdir"]],"!\n"))
}


if(opt[["dbname"]]%in%c("nr","refseq","swissprot"))
{
dl.ncbi_blastdb(what=opt[["dbname"]])
}else if(opt[["dbname"]]%in%c("uniref90","uniref100","uniprotKB","uniprotSP")){
dl.uniprot(what=opt[["dbname"]])
}else if(opt[["dbname"]]=="ncbi_taxonomy"){
dl.ncbi_tax()
}else if(grepl("^custom:.+:/",opt[["dbname"]],ignore.case=T))
{
import_custom_set(opt[["dbname"]])
}else
{
byedie(paste0("Database \'",opt[["dbname"]],"\' is not recognized!\nPlease consult the user manual for accepted database names!"))
}
info("Database update finished.")
